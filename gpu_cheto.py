#!/usr/bin/env python3
"""
person_clip_extractor.py ‚Äì RTX-4070 BEAST MODE üî•  (memory-safe edition, fixed device string)

Extract all segments where exactly **one** person is visible.
Optimised for high throughput on an RTX-4070 laptop while aggressively
protecting system RAM / VRAM.

Dependencies
------------
pip install ultralytics torch torchvision opencv-python face_recognition dlib-bin
# CUDA 11.8 wheels:
pip install --upgrade torch torchvision --index-url https://download.pytorch.org/whl/cu118
"""
from __future__ import annotations

import argparse, json, sys, time, gc, os, threading, multiprocessing as mp
from pathlib import Path
from typing import List, Dict, Iterable
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue  # noqa: F401  (kept for future use)

import psutil
import cv2
import numpy as np
import torch
from ultralytics import YOLO

# -----------------------------------------------------------------------------#
# Helper: accept 0, "0", "cuda:0", "cuda", "cpu" and return torch-friendly text #
# -----------------------------------------------------------------------------#
def _normalize_device(dev: str | int) -> str:
    """Return a canonical device string usable by torch."""
    if isinstance(dev, int) or (isinstance(dev, str) and dev.isdigit()):
        return f"cuda:{dev}"
    dev = str(dev).lower()
    if dev in {"cpu", "cuda"} or dev.startswith("cuda:"):
        return dev
    raise ValueError(f"Unsupported device spec: {dev!r}")

# ===========================  OPTIONAL FACE-ID SUPPORT  =======================
try:
    import face_recognition
    FACE_OK = True
except ImportError:
    FACE_OK = False
    print("[warn] face_recognition not available ‚Äì persistent IDs disabled", file=sys.stderr)

# ===========================  GPU OPTIMISER  ==================================
class GPUOptimizer:
    def __init__(self, device: str = "cuda:0"):
        self.device = _normalize_device(device)
        self.setup_beast_mode()

    def setup_beast_mode(self):
        if not torch.cuda.is_available() or not self.device.startswith("cuda"):
            return
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True
        torch.cuda.empty_cache()
        if hasattr(torch.cuda, "memory_pool"):
            torch.cuda.memory_pool.empty_cache()
        torch.cuda.set_per_process_memory_fraction(0.95)
        print("[üî•] BEAST MODE ACTIVATED for RTX-4070!")
        self.print_gpu_info()

    def print_gpu_info(self):
        if not torch.cuda.is_available():
            return
        idx = int(self.device.split(":")[1]) if ":" in self.device else 0
        props = torch.cuda.get_device_properties(idx)
        total = props.total_memory / 1024**3
        alloc = torch.cuda.memory_allocated(idx) / 1024**3
        reserved = torch.cuda.memory_reserved(idx) / 1024**3
        print(f"[GPU] {props.name}")
        print(f"[MEM] Total {total:.1f} GB | Alloc {alloc:.1f} GB | Reserved {reserved:.1f} GB")
        print(f"[CUDA] Cores {props.multi_processor_count} | CC {props.major}.{props.minor}")

# ==========================  FAST VIDEO READER =================================
class MemoryEfficientVideoReader:
    """Read frames with minimal seeks & RAM footprint."""

    def __init__(self, video_path: Path, skip_frames: int = 1, max_frames: int | None = None):
        self.video_path = video_path
        self.skip_frames = skip_frames
        self.cap: cv2.VideoCapture | None = None

        tmp = cv2.VideoCapture(str(video_path))
        self.fps = tmp.get(cv2.CAP_PROP_FPS) or 30
        total_frames = int(tmp.get(cv2.CAP_PROP_FRAME_COUNT))
        tmp.release()

        self.frame_indices = list(range(0, total_frames, skip_frames))
        if max_frames:
            self.frame_indices = self.frame_indices[:max_frames]
        print(f"[reader] {video_path.name}: {len(self.frame_indices)} frames")

    def __enter__(self):
        self.cap = cv2.VideoCapture(str(self.video_path))
        return self

    def __exit__(self, *_):
        if self.cap:
            self.cap.release()
            self.cap = None

    def read_batch(self, start_idx: int, batch_size: int):
        frames, indices = [], []
        end_idx = min(start_idx + batch_size, len(self.frame_indices))
        for i in range(start_idx, end_idx):
            fidx = self.frame_indices[i]
            self.cap.set(cv2.CAP_PROP_POS_FRAMES, fidx)
            ok, frame = self.cap.read()
            if ok:
                frames.append(frame)
                indices.append(fidx)
        return frames, indices

    def __len__(self):
        return len(self.frame_indices)

# ==========================  MEMORY MANAGER ====================================
class MemoryManager:
    @staticmethod
    def cleanup_all():
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

    @staticmethod
    def get_memory_usage():
        proc = psutil.Process(os.getpid())
        ram_mb = proc.memory_info().rss / 1024 / 1024
        vram_mb = torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0
        return ram_mb, vram_mb

    @staticmethod
    def check_memory_limit(max_ram_gb: float = 8.0):
        ram_mb, _ = MemoryManager.get_memory_usage()
        if ram_mb / 1024 > max_ram_gb:
            print(f"[‚ö†Ô∏è] High RAM ({ram_mb/1024:.1f} GB) ‚Äì forcing cleanup")
            MemoryManager.cleanup_all()
            return True
        return False

# ==========================  CLIP UTILITIES  ===================================
def _ms(frame: int, fps: float) -> int:           # frame index ‚Üí milliseconds
    return int(frame / fps * 1000)

def _match_fast(enc, known: List[Dict], next_id: list[int], tol: float = 0.6) -> int:
    """Vectorised face-embedding matcher (optional)."""
    if enc is None or not known or not FACE_OK:
        pid = next_id[0]
        next_id[0] += 1
        return pid

    if len(known) > 10:
        encs = np.array([k["enc"] for k in known])
        dists = np.linalg.norm(encs - enc, axis=1)
        idx = np.argmin(dists)
        if dists[idx] <= tol:
            return known[idx]["id"]
    else:
        matches = face_recognition.compare_faces([k["enc"] for k in known], enc, tolerance=tol)
        if True in matches:
            return known[matches.index(True)]["id"]

    pid = next_id[0]
    known.append({"id": pid, "enc": enc})
    next_id[0] += 1
    return pid

def _filter_and_merge_clips(clips: List[Dict], min_dur_ms: int = 100, gap_ms: int = 500):
    if not clips:
        return []
    clips = [c for c in clips if (c["end_ms"] - c["start_ms"]) >= min_dur_ms]
    if not clips:
        return []
    clips.sort(key=lambda x: (x["video_id"], x["person_id"], x["start_ms"]))
    merged, cur = [], None
    for c in clips:
        if (cur is None or c["video_id"] != cur["video_id"] or
            c["person_id"] != cur["person_id"] or c["start_ms"] - cur["end_ms"] > gap_ms):
            if cur is not None:
                merged.append(cur)
            cur = c.copy()
        else:
            cur["end_ms"] = c["end_ms"]
    if cur is not None:
        merged.append(cur)
    return merged

# ==========================  MAIN PROCESSOR  ===================================
class RTX4070BeastProcessor:
    """Wraps a YOLO v8 model with memory-safe per-batch logic."""

    def __init__(self, weights: str, device: str = "cuda:0", conf: float = 0.25):
        self.device = _normalize_device(device)
        self.conf = conf
        self.gpu_opt = GPUOptimizer(self.device)
        self.model = self._load_model(weights)

    def _load_model(self, weights: str):
        print("[üöÄ] Loading YOLO model‚Ä¶")
        model = YOLO(weights).to(self.device)
        if self.device != "cpu":
            try:
                model.model.half()
                print("[‚ö°] Half precision enabled")
            except Exception as e:
                print("[warn] Could not switch to FP16:", e)
            try:
                if hasattr(torch, "compile"):
                    model.model = torch.compile(model.model, mode="max-autotune",
                                                fullgraph=True, dynamic=False)
                    print("[üî•] Model compiled with max-autotune")
            except Exception as e:
                print("[warn] torch.compile failed:", e)
        return model

    # ------------------------------------------------------------------ #
    #                     MEMORY-SAFE VIDEO PROCESSING                   #
    # ------------------------------------------------------------------ #
    def process_video_memory_safe(
        self,
        video_path: Path,
        batch_size: int = 32,
        skip_frames: int = 2,
        max_frames: int | None = None,
        known: List[Dict] | None = None,
        next_id: list[int] | None = None,
    ) -> List[Dict]:
        if known is None:
            known = []
        if next_id is None:
            next_id = [1]

        print(f"\n[üî•] MEMORY-SAFE PROCESSING: {video_path.name}")
        segs: List[Dict] = []
        st = end = pid = None
        t0 = time.time()
        processed = 0

        with MemoryEfficientVideoReader(video_path, skip_frames, max_frames) as reader:
            batches = (len(reader) + batch_size - 1) // batch_size
            for b in range(batches):
                offset = b * batch_size
                frames, idxs = reader.read_batch(offset, batch_size)
                if not frames:
                    continue
                try:
                    with torch.cuda.amp.autocast(enabled=True):
                        res = self.model(
                            frames,
                            classes=[0], conf=self.conf, device=self.device,
                            verbose=False, agnostic_nms=True, max_det=5, half=True
                        )
                except Exception as e:
                    print(f"[error] Batch {b}: {e}")
                    del frames
                    MemoryManager.cleanup_all()
                    continue

                for r, fi in zip(res, idxs):
                    count = sum(int(box.cls[0]) == 0 for box in r.boxes) if r.boxes else 0
                    if count == 1:
                        if st is None:
                            pid = next_id[0]
                            next_id[0] += 1
                            st = int(fi)
                        end = int(fi)
                    else:
                        if st is not None:
                            segs.append({"person_id": pid,
                                         "start_ms": _ms(st, reader.fps),
                                         "end_ms": _ms(end, reader.fps)})
                            st = end = pid = None

                del frames, res
                processed += len(idxs)

                if b % 10 == 0:
                    MemoryManager.cleanup_all()
                    ram_mb, vram_mb = MemoryManager.get_memory_usage()
                    fps = processed / (time.time() - t0)
                    print(f"[‚ö°] Batch {b}/{batches}: {fps:.1f} fps | RAM {ram_mb:.0f} MB | VRAM {vram_mb:.0f} MB")
                    MemoryManager.check_memory_limit(max_ram_gb=6.0)

        if st is not None:
            segs.append({"person_id": pid,
                         "start_ms": _ms(st, reader.fps),
                         "end_ms": _ms(end, reader.fps)})

        MemoryManager.cleanup_all()
        dt = time.time() - t0
        print(f"[üèÅ] {video_path.name}: {processed} frames in {dt:.1f}s ({processed/dt:.1f} fps)")
        print(f"[üìä] Found {len(segs)} raw segments")
        return segs

# ==========================  THIN WRAPPERS FOR CLI  ===========================
def process_video_safe(
    video: Path,
    processor: RTX4070BeastProcessor,
    batch_size: int,
    skip_frames: int,
    max_frames: int | None,
    known: List[Dict],
    next_id: list[int],
) -> List[Dict]:
    clips = processor.process_video_memory_safe(
        video, batch_size, skip_frames, max_frames, known, next_id
    )
    for c in clips:
        c["video_id"] = video.stem
    return clips

def process_many_memory_safe(
    dir_in: Path,
    weights: str,
    conf: float,
    device: str,
    patterns: List[str],
    batch_size: int,
    skip_frames: int,
    max_frames: int | None,
    save_interval: int = 5,
) -> List[Dict]:
    vids: List[Path] = []
    for pat in patterns:
        vids.extend(dir_in.glob(pat))
    vids.sort()
    if not vids:
        print(f"No videos found in {dir_in}")
        return []

    print(f"[üéØ] Will process {len(vids)} videos")
    proc = RTX4070BeastProcessor(weights, device, conf)
    known: List[Dict] = []
    next_id = [1]
    clips_all: List[Dict] = []

    for i, vid in enumerate(vids, 1):
        clips = process_video_safe(
            vid, proc, batch_size, skip_frames, max_frames, known, next_id
        )
        clips_all.extend(clips)
        if i % save_interval == 0:
            print(f"[üíæ] Checkpoint after {i} videos ‚Äì {len(clips_all)} clips so far")

    return _filter_and_merge_clips(clips_all)

# ==========================  COMMAND-LINE INTERFACE  ==========================
def _cli():
    ap = argparse.ArgumentParser(
        prog="person_clip_extractor.py",
        description="RTX-4070 BEAST MODE ‚Äì single-person clip extractor (memory-safe)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
üî• PRESETS
  --preset ultra   : Highest speed  (batch 48, skip 3)
  --preset beast   : Balanced       (batch 32, skip 2)
  --preset turbo   : Conservative   (batch 24, skip 1)
""",
    )
    # IO
    ap.add_argument("--input", help="Single video file")
    ap.add_argument("--input-dir", default="/media/linuxbida/EXTERNAL_USB/Editor_videos/data/videos")
    ap.add_argument("--out-file")
    ap.add_argument("--output-dir", default="/media/linuxbida/EXTERNAL_USB/Editor_videos/testing/output")
    # model / GPU
    ap.add_argument("--model", default="yolov8n.pt")
    ap.add_argument("--conf", type=float, default=0.25)
    ap.add_argument("--device", default="cuda:0",
                    help='GPU device: "cuda:0", "cuda:1", or "cpu"')
    ap.add_argument("--patterns", nargs="*", default=["*.mp4", "*.mov", "*.mkv", "*.avi", "*.m4v"])
    # filtering / processing
    ap.add_argument("--min-duration", type=int, default=100, help="Min clip length (ms)")
    ap.add_argument("--merge-gap", type=int, default=500, help="Max gap to merge (ms)")
    ap.add_argument("--batch-size", type=int, default=64)
    ap.add_argument("--skip-frames", type=int, default=2)
    ap.add_argument("--max-frames", type=int)
    ap.add_argument("--save-interval", type=int, default=5)
    ap.add_argument("--max-ram-gb", type=float, default=6.0)
    ap.add_argument("--preset", choices=["ultra", "beast", "turbo"])
    args = ap.parse_args()

    # presets
    if args.preset == "ultra":
        args.batch_size, args.skip_frames = 48, 3
        print("[üî•] ULTRA preset")
    elif args.preset == "beast":
        args.batch_size, args.skip_frames = 32, 2
        print("[‚ö°] BEAST preset")
    elif args.preset == "turbo":
        args.batch_size, args.skip_frames = 24, 1
        print("[üöÄ] TURBO preset")

    # system info
    print(f"[üíª] {mp.cpu_count()} CPU cores | {psutil.virtual_memory().total/1024**3:.1f} GB RAM")
    if torch.cuda.is_available():
        name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"[üéÆ] GPU: {name} ({vram:.1f} GB)")

    out_path = Path(args.out_file) if args.out_file else Path(args.output_dir) / "clips.json"
    out_path.parent.mkdir(parents=True, exist_ok=True)

    t0 = time.time()
    if args.input:
        proc = RTX4070BeastProcessor(args.model, args.device, args.conf)
        clips = process_video_safe(
            Path(args.input), proc,
            args.batch_size, args.skip_frames, args.max_frames,
            known=[], next_id=[1]
        )
        clips = _filter_and_merge_clips(clips, args.min_duration, args.merge_gap)
    else:
        clips = process_many_memory_safe(
            Path(args.input_dir), args.model, args.conf, args.device,
            args.patterns, args.batch_size, args.skip_frames,
            args.max_frames, args.save_interval
        )

    if clips:
        out_path.write_text(json.dumps(clips, indent=2))
        total_dur = sum(c["end_ms"] - c["start_ms"] for c in clips) / 1000
        print(f"\n[üèÜ] Saved {len(clips)} clips ‚Üí {out_path}")
        print(f"[üìä] Total single-person content: {total_dur:.1f} s")
    else:
        print("[‚ùå] No clips found")
    print(f"[‚è±Ô∏è] Finished in {time.time()-t0:.1f}s")

if __name__ == "__main__":
    _cli()
